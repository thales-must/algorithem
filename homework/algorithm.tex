\documentclass{article}
\usepackage[top=1cm,left=1cm,right=1.5cm,bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\title{Algorithm Homework}
\author{Tai Jiang}
\date{October 2023}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{color}
\usepackage{listings}
\lstset{
 columns=fixed,       
 numbers=left,                                        % 在左侧显示行号
 language=c++,                                        % 设置语言
}

\begin{document}
  \maketitle
  \tableofcontents
  \pagenumbering{gobble}
  \newpage
  \pagenumbering{arabic}
  \section{Show that the solution of  $ T \left(n\right) = T \left(\left\lceil n/2 \right\rceil\right) + 1  $ is $ O \left( \lg n \right)  $ .}
    \paragraph{Origin}:
      \subparagraph{Exercise}4.3-2
      \subparagraph{Page}87
    \paragraph{Answer}:

  That can build a recursion tree to visualize how this recurrence works:

  \begin{enumerate}
    \item Start with $ T \left(n\right) $ at the top.
    \item At each level of the tree, we have $ T \left( \left\lceil n/2 \right\rceil \right) + 1 $.
    \item The tree branches into two subproblems, one with size $ \left\lceil n/2 \right\rceil $ and another with size $ \left\lceil n/2 \right\rceil $.
  \end{enumerate}

  The tree might look like Figure \ref{fig:tree1}.

  \begin{figure}[h!]
    \centering
    \begin{tikzpicture}
      [level distance=50px,
       level 1/.style={sibling distance=160px},
       level 2/.style={sibling distance=80px},
       level 3/.style={sibling distance=50px}]
      \node {$ T \left(\left\lceil n \right\rceil\right) $}
        child {node {$ T \left(\left\lceil n/2 \right\rceil\right) $}
          child {node {$ T \left(\left\lceil  \left(\left\lceil n/2 \right\rceil\right) /2 \right\rceil\right) $}
            child {node {...}}
            child {node {...}}
          }
          child {node {$ T \left(\left\lceil  \left(\left\lceil n/2 \right\rceil\right) /2 \right\rceil\right) $}
            child {node {...}}
            child {node {...}}
          }
        }
        child {node {$ T \left(\left\lceil n/2 \right\rceil\right) $}
          child {node {$ T \left(\left\lceil  \left(\left\lceil n/2 \right\rceil\right) /2 \right\rceil\right) $}
            child {node {...}}
            child {node {...}}
          }
          child {node {$ T \left(\left\lceil  \left(\left\lceil n/2 \right\rceil\right) /2 \right\rceil\right) $}
            child {node {...}}
            child {node {...}}
          }
        };
    \end{tikzpicture}
    \caption{Recursion Tree.}
    \label{fig:tree1}
  \end{figure}

  At each level, the size of the problem is divided by 2, and we keep going until the size becomes 1 (or less, in which case we stop). The depth of the tree will be the number of times we can halve n until it becomes 1. So it's the number of times we can take $ \left\lceil n/2 \right\rceil $ until $ \left\lceil n/2 \right\rceil \leq  1 $.
  
  At each step, we're taking the ceiling of half of the previous value, which is effectively dividing it by 2. We continue this process until the value is less than or equal to 1. So, let k be the number of steps it takes for $ \left\lceil n/2 \right\rceil $ to become 1, look like equation \eqref{eq:tree1}.

  \begin{equation}
    \frac{\left\lceil n/2 \right\rceil}{2^k}  \leq 1
    \label{eq:tree1}
  \end{equation}

  Then sovle for k, look like equation \eqref{eq:tree2}:
  
  \begin{equation}
    \begin{aligned}
      \frac{n}{2^k} \leq 1 \\
      n \leq 2^k \\
      \lg n \leq k
    \end{aligned}
    \label{eq:tree2}
  \end{equation}

  The depth of the recursion tree is $ O(\lg n) $, which means the algorithm has a time complexity of $ O(\lg n) $. So the solution of  $ T \left(n\right) = T \left(\left\lceil n/2 \right\rceil\right) + 1 $ is indeed $ O(\lg n) $.

  \section{Use a recursion tree to determine a good asymptotic upper bound on the recurrence $ T \left(n\right) = 4T \left( n/2 + 2 \right) + n  $. Use the substitution method to verify your answer.}
    \paragraph{Origin}:
      \subparagraph{Exercise}4.4-3
      \subparagraph{Page}93
    \paragraph{Answer}:

    That can build a recursion tree to visualize how this recurrence works:

    \begin{enumerate}
      \item Start with $ T \left(n\right)  $ at the top.
      \item At each level of the tree, we have 4 subproblems of size $ T \left( n/2 + 2 \right) + n  $.
      \item The cost of each level is n.
    \end{enumerate}

    The tree might look like Figure \ref{fig:tree2}.

  \begin{figure}[h!]
    \centering
    \begin{tikzpicture}[sibling distance=50px]
      \node {T(n)}
        child {node {T(n/2+2)}
          child {node {n}}
        }
        child {node {T(n/2+2)}
          child {node {n}}
        }
        child {node {T(n/2+2)}
          child {node {n}}
        }
        child {node {T(n/2+2)}
          child {node {n}}
        };

    \end{tikzpicture}
    \caption{Recursion Tree.}
    \label{fig:tree2}
  \end{figure}

  At each level, we have 4 subproblems of size T(n/2 + 2), and each subproblem incurs a cost of n. The number of levels in the tree will depend on how quickly the subproblem size decreases.

  The subproblem size is equation \eqref{eq:q2_1}:

  \begin{equation}
    T(n) = 4T(n/2 + 2) + n
    \label{eq:q2_1}
  \end{equation}

  The subproblem size is n/2 + 2, so we calculate the subproblem size for the next level by equation \eqref{eq:q2_2}:
  \begin{equation}
    \begin{aligned}
      T(n/2 + 2) &= 4T((n/2 + 2)/2 + 2) + n/2 + 2 \\
      &= 4T(n/4 + 1 + 2) + n/2 + 2 \\
      &= 4T(n/4 + 3) + n/2 + 2      
    \end{aligned}
    \label{eq:q2_2}
  \end{equation}

  At each level, we are adding 2 to the subproblem size. Therefore, at level k, the subproblem size will be $ n/(2^k) + 2k $.

  Now, we want to find the level where the subproblem size becomes a constant, $ n/(2^k) + 2k = C for some constant C $, calculate the k look like equation \eqref{eq:q2_3}.

  \begin{equation}
    \begin{aligned}
      n/(2^k) + 2k &= C \\
      n/(2^k) &= C - 2k \\
      2^k &= n/(C - 2k) \\
      k &= \lg (n/(C - 2k))
    \end{aligned}
    \label{eq:q2_3}
  \end{equation}

  C is a constant. The number of levels in the recursion tree is $ O(\lg n) $.

  The cost at each level in the recursion tree is equation \eqref{eq:q2_4}:

  \begin{equation}
    \begin{aligned}
      \text{At level 0:}& \quad n \\
      \text{At level 1:}& \quad 4 * n/2 = 2n \\
      \text{At level 2:}& \quad 4 * (n/4) = n \\
      ... \\
      \text{At level k:}& \quad (4^k) * (n/(2^k)) = (n/2^k) * (4^k) = C * 4^k
    \end{aligned}
    \label{eq:q2_4}
  \end{equation}

  Summing up the costs of all levels(Equation \eqref{eq:q2_5}):

  \begin{equation}
    T(n) = n + 2n + 4n + ... + C * 4^k
    \label{eq:q2_5}
  \end{equation}

  This is a geometric series, and its sum can be bounded by(Equation \eqref{eq:q2_6}):

  \begin{equation}
    T(n) \leq  n * (1 - 4^(k+1))/(1 - 4)
    \label{eq:q2_6}
  \end{equation}

  Since $k = O(\lg(n))$, $4^(k+1)$ is polynomial in n. Therefore, T(n) is O(n).

  verify this result using the substitution method:

  Assume that $T(m) \leq  km - p$ for some positive constants k and p, where m < n(Equation \eqref{eq:q2_7}).

  \begin{equation}
    \begin{aligned}
      T(n) = 4T(n/2 + 2) + n \\
        \leq  4(k(n/2 + 2) - p) + n \\
        = 2kn - 4k + 4n - 4p + n \\
        = (2k + 5)n - 4k - 4p
    \end{aligned}
    \label{eq:q2_7}
  \end{equation}

  Find k and p such that $(2k + 5)n - 4k - 4p \leq  kn - p$.
  
  This holds if $2k + 5 \leq k and -4k - 4p \leq -p$.
  
  Solving these inequalities(Equation \eqref{eq:q2_8}):

  \begin{equation}
    \begin{aligned}
      2k + 5 &\leq k \\
      k &\leq -5 \\
      -4k - 4p &\leq -p \\
      -4k &\leq 0\\
      k &\geq  0
    \end{aligned}
    \label{eq:q2_8}
  \end{equation}

  Since we can't find k and p that satisfy these inequalities, our initial assumption that $ T(m) \leq km - p$ for m < n is incorrect.

  Therefore, T(n) is not $O(n^k)$ for any positive constant k. Instead, as shown earlier, T(n) is O(n).
  
  \section{Can the master method be applied to the recurrence $ T(n) = 4T(n/2) + n^2 \lg n $? Why or why not? Give an asymptotic upper bound for this recurrence.}

  \paragraph{Origin}:
    \subparagraph{Exercise}4.5-4
    \subparagraph{Page}97
  \paragraph{Answer}:

  The master theorem can be applied to recurrence

  To apply the master theorem, we need to check whether f(n) satisfies the following conditions for some constant $\epsilon  > 0$:

  \begin{enumerate}
    \item If $ f(n) = O(n^{log_{b} a - \epsilon } ) $, for some $\epsilon > 0$, then $T(n) = \Theta (n^{log_{b}a})$.
    \item If $ f(n) = \Theta(n^{log_{b}a}) $, then $ T(n) = \Theta(n^{log_{b} a}  \lg n) $.
    \item If $ f(n) = \Omega (n^{log_{b}a + \epsilon}) $, for some $\epsilon > 0$, and if $a f(n/b) \leq k * f(n)$ for some k < 1 and sufficiently large n, then $T(n) = \Theta(f(n))$.
  \end{enumerate}

  The asymptotic upper bound for the given recurrence relation $T(n) = 4T(n/2) + n^2 * \lg n = O(n^2)$.

  \section{Use indicator random variables to compute the expected value of the sum of n dice.}

  \paragraph{Origin}:
    \subparagraph{Exercise}5.2-3
    \subparagraph{Page}122
  \paragraph{Answer}:

Let $X_i$ be the random variable that is 1 if the i-th die shows a particular face (1, 2, 3, 4, 5, or 6), and 0 otherwise.

The sum of n dice can then be expressed as the sum of these indicator variables:

$ S = X_1 + X_2 + X_3 + ... + X_n $

Now, we can calculate the expected value of S:

$ E(S) = E(X_1) + E(X_2) + E(X_3) + ... + E(X_n) $

Since each die is fair, the probability of each face (1, 2, 3, 4, 5, or 6) appearing on a single die is 1/6, and the expected value of each indicator variable is:

$ E(X_i) = 1 * P(X_i = 1) + 0 * P(X_i = 0) = 1 * (1/6) + 0 * (5/6) = 1/6 $

Now, you can sum up the expected values of the individual indicators:

$ E(S) = E(X_1) + E(X_2) + E(X_3) + ... + E(X_n) = (1/6) + (1/6) + (1/6) + ... + (1/6) = (n/6) $

So, the expected value of the sum of n dice is (n/6).



  \section{Use indicator random variables to solve the following problem, which is known as the hat-check problem. Each of n customers gives a hat to a hat-check person at a restaurant. The hat-check person gives the hats back to the customers in a random order. What is the expected number of customers who get back their own hat?}

  \paragraph{Origin}:
    \subparagraph{Exercise}5.2-4
    \subparagraph{Page}122
  \paragraph{Answer}:

  Let $X_i$ be an indicator random variable for the i-th customer, where:

  $X_i = 1$ if the i-th customer gets their own hat back.
  $X_i = 0$ if the i-th customer does not get their own hat back.
  
  The probability that the i-th customer gets their own hat back is 1/n, as there are n hats and they are returned in a random order. Therefore, we have:
  
  $E(X_i) = P(X_i = 1) = 1/n$
  
  Now, let's define a random variable Y as the total number of customers who get their own hat back. Y is the sum of the indicator random variables for each customer:
  
  $Y = X_1 + X_2 + X_3 + ... + X_n$
  
  Now, we can find the expected value of Y using linearity of expectation:
  
  $E(Y) = E(X_1 + X_2 + X_3 + ... + X_n)$
  
  Using the linearity of expectation, we can write this as:
  
  $E(Y) = E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)$
  
  Since each customer's indicator random variable has the same expected value (1/n), we can simplify further:
  
  $E(Y) = (1/n) + (1/n) + (1/n) + ... + (1/n) (n times)$
  
  $E(Y) = (1/n) * n = 1$
  
  So, the expected number of customers who get back their own hat is 1. This result may be surprising, but it's a classic result of the hat-check problem. On average, one customer is expected to get their own hat back, while the others get hats belonging to other customers.
  \section{Show that the worst-case running time of HEAPSORT is $ \Omega (n \lg n)$.}
  \paragraph{Origin}:
    \subparagraph{Exercise}6.6-4
    \subparagraph{Page}160
  \paragraph{Answer}:

  Basic heapsort:

  \begin{enumerate}
    \item Build a max-heap from the input array.
    \item Repeatedly remove the maximum element from the heap and place it at the end of the array, shrinking the heap size.
    \item Repeat step 2 until the heap is empty.
  \end{enumerate}

  The worst-case occurs when the input array is specially ordered, i.e., the largest element is at the beginning, the second largest is at the second position, and so on.

  Every time we extract the maximum element from the heap (which is always at the root of the heap), we have to perform the following operations:

  \begin{enumerate}
    \item Swap the maximum element with the last element of the array.
    \item Restore the heap property, which involves down-heapifying the heap (sifting down the element that was originally at the end of the array, which is the smallest element in the heap)
  \end{enumerate}

  After the first element is removed, the second-largest element becomes the new root, and the rest of the array is unsorted. We then need to restore the heap property again and again for each element, resulting in a series of down-heapify operations.

  In the worst-case scenario, for each element, we perform a down-heapify operation on a heap of size n, and the number of operations increases as we progress through the array. The first element requires the most operations, the second element requires fewer operations, and so on.

  The total number of operations can be shown to be $\Omega (n \lg n)$ because each down-heapify operation takes $\Omega (\lg n)$ time, and we do this for all n elements in the worst-case scenario.

  Therefore, in the worst-case scenario, HEAPSORT has a lower bound of $\Omega (n \lg n)$, meaning that its worst-case running time is at least proportional to n lg n.

  \section{Give an $O (n \lg k)$-time algorithm to merge k sorted lists into one sorted list, where n is the total number of elements in all the input lists. (Hint: Use a minheap for k-way merging.)}
  \paragraph{Origin}:
    \subparagraph{Exercise}6.5-9
    \subparagraph{Page}166
  \paragraph{Answer}:
  
  \begin{enumerate}
    \item Create a min-heap and initialize it with the first element from each of the k sorted lists, along with an index indicating the list it came from. The heap will keep track of the smallest element among these k elements. The index is important for knowing which list the element belongs to.
    \item Initialize an empty result list to store the sorted elements.
    \item Repeat the following steps until the min-heap is empty:
    
    \begin{enumerate}
      \item Extract the minimum element (the smallest among the k elements) from the min-heap. This element came from one of the input lists.
      \item Add this element to the result list.
      \item Retrieve the next element from the same input list (the one we just extracted from) and add it to the min-heap. If there are no more elements in that list, do nothing.
    \end{enumerate}
    \item Continue these steps until the min-heap is empty. The result list will be the merged, sorted list.
  \end{enumerate}
  

  \section{Use the substitution method to prove that the recurrence $T (n) = T(n-1) + \Theta(n)$ has the solution $T(n) = \Theta(n^2)$, as claimed at the beginning of Section 7.2.}
  \paragraph{Origin}:
    \subparagraph{Exercise}7.2-1
    \subparagraph{Page}178
  \paragraph{Answer}:
  

To prove that the recurrence $T(n) = T(n - 1) + \Theta(n)$ has the solution $T(n) = \Theta(n^2)$ using the substitution method, we'll first make an educated guess and then use mathematical induction to prove it.

Guess: $T(n) = \Theta(n^2)$

Inductive Hypothesis: We assume that $T(k) = \Theta(k^2)$ for all k < n, where n is a positive integer.

Now, we will prove that $T(n) = \Theta(n^2)$ based on this assumption.

$T(n) = T(n - 1) + \Theta(n)$

By our inductive hypothesis, $T(n - 1) = \Theta((n - 1)^2)$, which we can write as $T(n - 1) = \Theta(n^2 - 2n + 1)$.

Now, let's substitute this into the original recurrence:

$T(n) = \Theta(n^2 - 2n + 1) + \Theta(n)$

Since $\Theta(n^2)$ dominates $\Theta(-2n + 1)$ and $\Theta(n)$ in terms of growth rates, we can drop the lower-order terms and constants, as they won't affect the asymptotic behavior:

$T(n) = \Theta(n^2)$

So, we've shown that for n, $T(n) = \Theta(n^2)$, assuming $T(k) = \Theta(k^2)$ for all k < n. This completes the proof by induction.

Therefore, the solution to the recurrence $T(n) = T(n - 1) + \Theta(n) is T(n) = \Theta(n^2)$.

  \section{Show that the running time of QUICKSORT is $\Theta(n ^ 2)$ when the array A contains distinct elements and is sorted in decreasing order.}
  \paragraph{Origin}:
    \subparagraph{Exercise}7.2-3
    \subparagraph{Page}178
  \paragraph{Answer}:
  
To show that the running time of QUICKSORT is $\Theta(n^2)$ when the array A contains distinct elements and is sorted in decreasing order, we can analyze the worst-case behavior of the algorithm. In this specific scenario, the worst-case behavior occurs when the pivot selection strategy consistently selects the smallest or largest element as the pivot, leading to unbalanced partitioning.

Here's a step-by-step analysis:

\begin{enumerate}
  \item In QUICKSORT, a pivot element is chosen, and the array is partitioned into two subarrays: elements less than the pivot and elements greater than the pivot.

  \item In the case where the array is already sorted in decreasing order, if the pivot selection strategy consistently chooses the largest element as the pivot, then one partition will contain n - 1 elements, and the other partition will contain only 1 element. This results in an unbalanced partition.
  
  \item Consequently, the algorithm makes n - 1 recursive calls to sort the subarray with n - 1 elements and only 1 recursive call to sort the subarray with 1 element.
  
  \item In the next level of recursion, the same situation occurs. The larger partition is further divided into two subarrays, one with n - 2 elements and the other with 1 element, and so on.
  
  \item The recursive calls continue, each time decreasing the size of the larger partition by 1, until the entire array is sorted.
  
  \item The total number of recursive calls made in this scenario is $1 + 2 + 3 + ... + (n - 1)$, which is a sum of the first n - 1 natural numbers. This sum is given by (n - 1)(n)/2.
\end{enumerate}

The number of comparisons made during each level of recursion is proportional to the size of the partition. In this worst-case scenario, the partition sizes are unbalanced, with one partition containing only 1 element. Therefore, the total number of comparisons is also proportional to (n - 1)(n)/2.

Asymptotically, we can conclude that the running time of QUICKSORT in this worst-case scenario, where the array is sorted in decreasing order, is $\Theta(n^2)$.

This demonstrates that when the array contains distinct elements and is sorted in decreasing order, the worst-case running time of QUICKSORT is $\Theta(n^2)$.


  \section{Stack depth for quicksort}
  \paragraph{Origin}:
    \subparagraph{Problems}7-4
    \subparagraph{Page}188
  \paragraph{Subject}:
  The QUICKSORT algorithm of Section 7.1 contains two recursive calls to itself. After QUICKSORT calls PARTITION, it recursively sorts the left subarray and then it recursively sorts the right subarray. The second recursive call in QUICKSORT is not really necessary; we can avoid it by using an iterative control structure. This technique, called \textit{tail recursion}, is provided automatically by good compilers. Consider the following version of quicksort, which simulates tail recursion:

  TAIL-RECURSIVE-QUICKSORT(A, p, r)

  \begin{lstlisting}
    while p<r
     // Partition and sort left subarray.
     q = PARTITION(A,p,r)
     TAIL-RECURSIVE-QUICKSORT(A,p,q-1)
     p=q+1
  \end{lstlisting}

  \begin{enumerate}
    \item[a] Argue that TAIL-RECURSIVE-QUICKSORT(A,1,A.length) correctly sorts the array A.
    

    Compilers usually execute recursive procedures by using a \textit{stack} that contains pertinent information, including the parameter values, for each recursive call. The information for the most recent call is at the top of the stack, and the information for the initial call is at the bottom. Upon calling a procedure, its information is \textit{pushed} onto the stack; when it terminates, its information is \textit{popped}.Sincewe assume that array parameters are represented by pointers, the information for each procedure call on the stack requires $O(1)$ stack space. The \textit{stack depth} is the maximum amount of stack space used at any time during a computation.
    
    
    \item[b] Describe a scenario in which TAIL-RECURSIVE-QUICKSORT's stack depth is $\Theta(n)$ on an n-element input array.
    \item[c] Modify the code for TAIL-RECURSIVE-QUICKSORT so that the worst-case stack depth is $\Theta(\lg n)$. Maintain the $O(n \lg n)$ expected running time of the algorithm.
  \end{enumerate}

  \section{Describe an algorithm that, given n integers in the range 0 to k, preprocesses its input and then answers any query about how many of the n integers fall into a range [a...b] in O(1) time. Your algorithm should use $\Theta(n+k)$ preprocessing time.}
  \paragraph{Origin}:
    \subparagraph{Exercise}8.2-4
    \subparagraph{Page}197
  \paragraph{Answer}:


To solve the problem of preprocessing n integers in the range 0 to k and answering queries about how many of the n integers fall into a given range [a...b] in O(1) time, you can use a technique called "prefix sum" or "cumulative sum" along with some preprocessing. Here's the algorithm:

\begin{enumerate}
  \item Create an array \textbf{counts} of size k + 1, initially filled with all zeros. This array will be used for preprocessing.
  \item Preprocessing ($\Theta(n)$ time):
  \begin{itemize}
    \item For each integer x in the input list, increment \textbf{counts[x]} by 1. This step will count the occurrences of each integer in the range 0 to k.
    \item Next, compute the cumulative sum array \textbf{cumulative} from the \textbf{counts} array. Initialize \textbf{cumulative[0]} with \textbf{counts[0]}, and for each i from 1 to k, set \textbf{cumulative[i] = cumulative[i-1] + counts[i]}. This step helps us obtain the cumulative count of integers less than or equal to i.
  \end{itemize}
  \item Query (O(1) time):
  \begin{itemize}
    \item To answer a query about how many integers fall into the range [a...b], use the cumulative sum array: \textbf{result = cumulative[b] - cumulative[a-1]}. This is a constant-time operation as you only need to perform two array lookups and subtraction.
  \end{itemize}
  \item This algorithm has a preprocessing time of $\Theta(n + k)$ and can answer queries in O(1) time, making it efficient for this specific problem. It effectively utilizes the cumulative sum array to provide quick answers to range count queries without reevaluating the counts for each query.
\end{enumerate}

\section{Show that the second smallest of n elements can be found with $n+\lceil \lg n\rceil -2$ comparisons in the worst case. (Hint: Also find the smallest element.)}
\paragraph{Origin}:
  \subparagraph{Exercise}9.1-1
  \subparagraph{Page}215
\paragraph{Answer}:


To find the second smallest of n elements and the smallest element using ($n + \lceil \lg n \rceil - 2$) comparisons in the worst case, you can use a modified version of the "tournament" method. This approach requires a total of $n + \lceil \lg n \rceil - 2$ comparisons in the worst case.

Here's how it works:

\begin{enumerate}
  \item Divide the n elements into pairs. Compare each element in each pair and keep track of the smaller element in each pair. This step requires n/2 comparisons.
  \item Continue dividing the winners of each pair into pairs and comparing them until you have just one element left. This will take $\lceil \lg n \rceil - 1$ more rounds of comparisons.
  \item At the end of this process, you will have the smallest element (found after n/2 comparisons) and $\lceil \lg n\rceil - 1$ other elements.
  \item Compare these $\lceil \lg n\rceil - 1$ elements to find the second smallest element. This step requires $\lceil \lg n\rceil - 1$ comparisons.

\end{enumerate}

The total number of comparisons required is $(n/2) + (\lceil \lg n \rceil - 1) = n/2 + \lceil \lg n \rceil - 1$, which is equivalent to $n + \lceil \lg n \rceil - 2$. Therefore, you can find the second smallest element with ($n + \lceil \lg n \rceil - 2$) comparisons in the worst case.

\section{In the algorithm SELECT, the input elements are divided into groups of 5. Will the algorithm work in linear time if they are divided into groups of 7? Argue that SELECT does not run in linear time if groups of 3 are used.}
\paragraph{Origin}:
  \subparagraph{Exercise}9.3-1
  \subparagraph{Page}223
\paragraph{Answer}:
Answer

\section{Professor Olay is consulting for an oil company, which is planning a large pipeline running east to west through an oil field of n wells. The company wants to connect a spur pipeline from each well directly to the main pipeline along a shortest route (either north or south), as shown in Figure \ref{fig:oilcompany}. Given the x-andy-coordinates of the wells, how should the professor pick the optimal location of the main pipeline, which would be the one that minimizes the total length of the spurs? Show how to determine the optimal location in linear time.}

\begin{figure}
  \includegraphics{oilcompany.png}
  \caption{Professor Olay needs to determine the position of the east-west oil pipeline that minimizes the total length of the north-south spurs.}
  \label{fig:oilcompany}
\end{figure}

\paragraph{Origin}:
  \subparagraph{Exercise}9.3-9
  \subparagraph{Page}223
\paragraph{Answer}:


Answer


\end{document}